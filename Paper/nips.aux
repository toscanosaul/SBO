\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kushner}
\citation{Mockus:1978}
\citation{Mockus:1989}
\citation{jones1998efficient}
\citation{Mockus:1989}
\citation{snoek2012practical}
\citation{forrester2008engineering}
\citation{brochu2010tutorial}
\citation{villemonteix2009informational}
\citation{huang2006global}
\citation{scott2011correlated}
\citation{brochu2010tutorial}
\citation{brochu2010tutorial}
\citation{glasserman2003monte}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{eq:goal}{{1}{1}{Introduction}{equation.1.1}{}}
\citation{o1991bayes}
\citation{Ho66}
\citation{glasserman2003monte}
\citation{Mockus:1989}
\citation{jones1998efficient}
\citation{Frazier:bayesianOpt}
\citation{frazier2009knowledge}
\citation{scott2011correlated}
\citation{Xie:2012:blind}
\newlabel{eq:strata}{{2}{2}{Introduction}{equation.1.2}{}}
\citation{RaWi06}
\citation{murphy2012machine}
\citation{RaWi06}
\citation{o1991bayes}
\citation{Ho66}
\@writefile{toc}{\contentsline {section}{\numberline {2}Statistical Model}{3}{section.2}}
\newlabel{model}{{2}{3}{Statistical Model}{section.2}{}}
\newlabel{eq:a_n}{{3}{3}{Statistical Model}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Stratified Bayesian Optimization (SBO) Algorithm}{3}{section.3}}
\newlabel{SBO}{{3}{3}{Stratified Bayesian Optimization (SBO) Algorithm}{section.3}{}}
\newlabel{eq:VOI}{{5}{4}{Stratified Bayesian Optimization (SBO) Algorithm}{equation.3.5}{}}
\newlabel{eq:max_VOI}{{6}{4}{Stratified Bayesian Optimization (SBO) Algorithm}{equation.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{SBO Algorithm}{4}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Computation of the Value of Information}{4}{section.4}}
\newlabel{sec:VOI}{{4}{4}{Computation of the Value of Information}{section.4}{}}
\newlabel{eq:VOI_a}{{7}{4}{Computation of the Value of Information}{equation.4.7}{}}
\citation{frazier2009knowledge}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Formulas for $\mathaccentV {tilde}07E{\sigma }_{n}\left (x,x_{n+1},w_{n+1}\right )$ and $a_{n}\left (x\right )$ }{5}{subsection.4.1}}
\citation{frazier2009knowledge}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{6}{section.5}}
\newlabel{experiments}{{5}{6}{Numerical Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Analytic Test Problem}{6}{subsection.5.1}}
\newlabel{sec:test}{{5.1}{6}{Analytic Test Problem}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  ({\bf  First row}) Left shows the contours of $F\left (x,w\right )=-x^{2}-w$. Right shows the contours of SBO's estimate of $F$. SBO builds a statistical model of $F(x,w)$, which implies a statistical model on $G(x)$, rather than building a model on $G(x)$ directly like other Bayesian optimization methods. ({\bf  Second row}) Left shows the contours of SBO's value of information, which depends on both $x$ and $w$, which SBO uses to choose the pair $(x,w)$ to sample next. Right shows SBO's estimates of $G(x)$, which is based on the estimate of $F(x,w)$ in the first row. ({\bf  Third row}) Left shows KG's value of information, which depends only on $x$, which KG uses to choose the point $x$ to sample next. Right shows KG's estimates of $G(x)$. This estimate is of lower quality than SBO's estimate above, because it does not use the observed values of $w$. \relax }}{7}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tahi10}{{1}{7}{({\bf First row}) Left shows the contours of $F\left (x,\w \right )=-x^{2}-\w $. Right shows the contours of SBO's estimate of $F$. SBO builds a statistical model of $F(x,\w )$, which implies a statistical model on $G(x)$, rather than building a model on $G(x)$ directly like other Bayesian optimization methods. ({\bf Second row}) Left shows the contours of SBO's value of information, which depends on both $x$ and $\w $, which SBO uses to choose the pair $(x,\w )$ to sample next. Right shows SBO's estimates of $G(x)$, which is based on the estimate of $F(x,\w )$ in the first row. ({\bf Third row}) Left shows KG's value of information, which depends only on $x$, which KG uses to choose the point $x$ to sample next. Right shows KG's estimates of $G(x)$. This estimate is of lower quality than SBO's estimate above, because it does not use the observed values of $\w $. \relax }{figure.caption.2}{}}
\citation{citibike}
\newlabel{fig:tahi7}{{2a}{8}{Performance comparison between SBO and a Bayesian optimization benchmark, the KG method, on the analytic test problem from Section~\ref {sec:test} \relax }{figure.caption.3}{}}
\newlabel{sub@fig:tahi7}{{a}{8}{Performance comparison between SBO and a Bayesian optimization benchmark, the KG method, on the analytic test problem from Section~\ref {sec:test} \relax }{figure.caption.3}{}}
\newlabel{fig:citibike}{{2b}{8}{Performance of SBO, on the Citi Bike Problem from Section~\ref {sec:citibike} \relax }{figure.caption.3}{}}
\newlabel{sub@fig:citibike}{{b}{8}{Performance of SBO, on the Citi Bike Problem from Section~\ref {sec:citibike} \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance results for SBO on the analytic test problem (plot a), the Citi Bike problem (plot b), and a screenshot from our simulation of the Citi Bike problem (plot c). \relax }}{8}{figure.caption.3}}
\newlabel{fig:stuff}{{2}{8}{Performance results for SBO on the analytic test problem (plot a), the Citi Bike problem (plot b), and a screenshot from our simulation of the Citi Bike problem (plot c). \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}New York City Citi Bike System}{8}{subsection.5.2}}
\newlabel{sec:citibike}{{5.2}{8}{New York City Citi Bike System}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Simulated Problems}{8}{subsection.5.3}}
\newlabel{sec:GPexample}{{5.3}{8}{Simulated Problems}{subsection.5.3}{}}
\newlabel{fig:sim1}{{3a}{9}{Percentage increase between SBO and a Bayesian optimization benchmark, the KG method, when the difference between variances is $0.5$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sim1}{{a}{9}{Percentage increase between SBO and a Bayesian optimization benchmark, the KG method, when the difference between variances is $0.5$\relax }{figure.caption.4}{}}
\newlabel{fig:sim2}{{3b}{9}{Percentage increase between SBO and a Bayesian optimization benchmark, the KG method, when the difference between variances is $1/1024$ \relax }{figure.caption.4}{}}
\newlabel{sub@fig:sim2}{{b}{9}{Percentage increase between SBO and a Bayesian optimization benchmark, the KG method, when the difference between variances is $1/1024$ \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance results for SBO on simulated problems when the difference between variances is large enough (plot a), Performance results for SBO on simulated problems when the difference between variances is small (plot b). \relax }}{9}{figure.caption.4}}
\newlabel{fig:simulated}{{3}{9}{Performance results for SBO on simulated problems when the difference between variances is large enough (plot a), Performance results for SBO on simulated problems when the difference between variances is small (plot b). \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}}
\newlabel{conclusion}{{6}{9}{Conclusion}{section.6}{}}
\bibstyle{unsrt}
\bibdata{nips}
\bibcite{kushner}{1}
\bibcite{Mockus:1978}{2}
\bibcite{Mockus:1989}{3}
\bibcite{jones1998efficient}{4}
\bibcite{snoek2012practical}{5}
\bibcite{forrester2008engineering}{6}
\bibcite{brochu2010tutorial}{7}
\bibcite{villemonteix2009informational}{8}
\bibcite{huang2006global}{9}
\bibcite{scott2011correlated}{10}
\bibcite{glasserman2003monte}{11}
\bibcite{o1991bayes}{12}
\bibcite{Ho66}{13}
\bibcite{Frazier:bayesianOpt}{14}
\bibcite{frazier2009knowledge}{15}
\bibcite{Xie:2012:blind}{16}
\bibcite{RaWi06}{17}
\bibcite{murphy2012machine}{18}
\bibcite{citibike}{19}
